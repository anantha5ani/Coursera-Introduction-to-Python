{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningWithPython_Chapter6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPw/keMUgOipPy6HIv+VfX4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anantha5ani/Coursera-Introduction-to-Python/blob/master/DeepLearningWithPython_Chapter6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZPq3CeWBQQ8",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 6 - Deep learning for text and sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsdvXhSdD8bu",
        "colab_type": "code",
        "outputId": "8366d646-0fa7-4abe-8ed7-f5c93233e779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "# set tensorflow to version 2\n",
        "%tensorflow_version 2.x # run this step everytime the kernel is restarted\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x # run this step everytime the kernel is restarted`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pyRooc8BLwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word-level one-hot encoding (toy example)\n",
        "import numpy as np\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index) + 1\n",
        "\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros(shape=(len(samples),\n",
        "                          max_length,\n",
        "                          max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = token_index.get(word)\n",
        "    results[i, j, index] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLLdaaGzCUVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Character-level on-hot encoding (toy example)\n",
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "characters = string.printable\n",
        "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character in enumerate(sample):\n",
        "    index = token_index.get(character)\n",
        "    results[i, j, index] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAIM4y8KCWqY",
        "colab_type": "code",
        "outputId": "0cb0541b-f3b5-4a38-c036-a5a80200bb5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Using Keras for word-level one-hot encoding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3OIOlW_DWCK",
        "colab_type": "code",
        "outputId": "f02f3b4e-b400-43bd-d32d-f7fbb6237807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "word_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ate': 7,\n",
              " 'cat': 2,\n",
              " 'dog': 6,\n",
              " 'homework': 9,\n",
              " 'mat': 5,\n",
              " 'my': 8,\n",
              " 'on': 4,\n",
              " 'sat': 3,\n",
              " 'the': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgGop4ChE9oT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word-level one-hot encoding with hashing trick (toy example)\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "      index = abs(hash(word)) % dimensionality\n",
        "      results[i, j, index] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-_oVLf4FURB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiating an Embedding layer\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(1000, 64)\n",
        "### The Embedding layer takes at least two arguments: \n",
        "##### the number of possible tokens (here, 1,000: 1 + maximum word index)\n",
        "##### and the dimensionality of the embeddings(here, 64)."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJTWNLLtFU3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the IMDB data for use with an Embedding layer\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "max_features = 10000 # Number of words to consider as features\n",
        "maxlen = 20 # Cuts off the text after this number of words (among the max_features most common words)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
        "    num_words=max_features) # Loads the data as lists of integers\n",
        "\n",
        "#### Turns the lists of integers into a 2D integer tensor of shape\n",
        "(samples, maxlen)\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUAQElT-HKmw",
        "colab_type": "code",
        "outputId": "e60e2607-cec7-4c23-fcf6-6b659c758fa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "source": [
        "# Using an Embedding layer and classifier on the IMDB data\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.6638 - acc: 0.6338 - val_loss: 0.6088 - val_acc: 0.7028\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.5329 - acc: 0.7546 - val_loss: 0.5232 - val_acc: 0.7334\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4583 - acc: 0.7903 - val_loss: 0.5000 - val_acc: 0.7486\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4201 - acc: 0.8123 - val_loss: 0.4961 - val_acc: 0.7536\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3939 - acc: 0.8256 - val_loss: 0.4963 - val_acc: 0.7560\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3718 - acc: 0.8386 - val_loss: 0.5010 - val_acc: 0.7562\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3527 - acc: 0.8485 - val_loss: 0.5043 - val_acc: 0.7578\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3345 - acc: 0.8605 - val_loss: 0.5105 - val_acc: 0.7560\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3170 - acc: 0.8694 - val_loss: 0.5187 - val_acc: 0.7538\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3003 - acc: 0.8778 - val_loss: 0.5261 - val_acc: 0.7490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJnNlCSMNMKX",
        "colab_type": "text"
      },
      "source": [
        "Download imdb raw dataset to build a model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLZn9aRJH9W_",
        "colab_type": "code",
        "outputId": "a97cc3af-e1a8-4fd7-e6c6-0ef9e1b49f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "### Uncomment blocks below if running for the first time - currently the datasets are already downloaded in place\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMa1KP4xN_FB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/kaggle/imdb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq-DggdFNR_L",
        "colab_type": "code",
        "outputId": "87b9540a-7296-409b-ec1e-091a1fda84de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "### extract zip files into folders\n",
        "!unzip -q aclImdb.zip -d aclImdb/  #unzip data in aclImdb/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace aclImdb/aclImdb/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/__MACOSX/aclImdb/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/aclImdb/test/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/__MACOSX/aclImdb/test/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/aclImdb/test/neg/0_2.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/__MACOSX/aclImdb/test/neg/._0_2.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/aclImdb/test/neg/10000_4.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/__MACOSX/aclImdb/test/neg/._10000_4.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/aclImdb/test/neg/10001_1.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/__MACOSX/aclImdb/test/neg/._10001_1.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/aclImdb/test/neg/10002_3.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/__MACOSX/aclImdb/test/neg/._10002_3.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/aclImdb/test/neg/10003_3.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/__MACOSX/aclImdb/test/neg/._10003_3.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/aclImdb/test/neg/10004_2.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/__MACOSX/aclImdb/test/neg/._10004_2.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/aclImdb/test/neg/10005_2.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/__MACOSX/aclImdb/test/neg/._10005_2.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace aclImdb/aclImdb/test/neg/10006_2.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlfbTqlxTcpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Processing the labels of the raw IMDB data\n",
        "import os\n",
        "\n",
        "imdb_dir = os.path.join(os.getcwd(), 'aclImdb/aclImdb')\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == '.txt':\n",
        "      f = open(os.path.join(dir_name, fname))\n",
        "      texts.append(f.read())\n",
        "      f.close()\n",
        "      if label_type == 'neg':\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta7SOE9dTgUs",
        "colab_type": "code",
        "outputId": "52eaf61e-f190-4152-bb2c-c2b00cf9d321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aclImdb.zip', 'aclImdb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFkSOsxCTjgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing the text of the raw IMDB data\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100\n",
        "training_samples = 200\n",
        "validation_samples = 10000\n",
        "max_words = 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}